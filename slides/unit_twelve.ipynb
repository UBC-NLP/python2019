{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "* Logistics\n",
    "* Quick Revision (5 mins)\n",
    "* Text Processing (Up until 10:20)\n",
    "* Break: 10:30 am â€“ 10:43 am\n",
    "* In-class practice: 10:43 am - 11:15 am\n",
    "* Final project discussions 11:15 - 11:50 am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistics \n",
    "* Homework 3 grading\n",
    "* Extra office hours today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Revision\n",
    "* What did we cover last time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: top\n",
      "Cleaned word: t^p\n"
     ]
    }
   ],
   "source": [
    "word= \"top\"\n",
    "new_word= word.translate(str.maketrans(\"o\",\"^\"))# str -> class in Pyton 3\n",
    "print(\"Original word:\", word)\n",
    "print(\"Cleaned word:\", new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: food\n",
      "Cleaned word: f^^d\n"
     ]
    }
   ],
   "source": [
    "word= \"food\"\n",
    "new_word= word.translate(str.maketrans(\"o\",\"^\"))\n",
    "print(\"Original word:\", word)\n",
    "print(\"Cleaned word:\", new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: jumping\n",
      "Cleaned word: jumper!\n"
     ]
    }
   ],
   "source": [
    "word= \"jumping\"\n",
    "new_word= word.translate(str.maketrans(\"ing\",\"er!\"))\n",
    "print(\"Original word:\", word)\n",
    "print(\"Cleaned word:\", new_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Handling Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: Hey, I really enjoyed the food :) #happy!\n",
      "Cleaned tweet: Hey I re^lly enj^yed the f^^d  h^ppy\n"
     ]
    }
   ],
   "source": [
    "tweet=\"Hey, I really enjoyed the food :) #happy!\"\n",
    "punc=string.punctuation\n",
    "cleaned=tweet.translate(str.maketrans(\"\",\"\", punc))\n",
    "print(\"Original tweet:\", tweet)\n",
    "print(\"Cleaned tweet:\",cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: Hey, I really enjoyed the food :) #happy!\n",
      "Cleaned tweet: Hey, I really enj^yed the f^^d :) #happy!\n"
     ]
    }
   ],
   "source": [
    "cleaned=tweet.translate(str.maketrans(\"o\",\"^\", punc))\n",
    "print(\"Original tweet:\", tweet)\n",
    "print(\"Cleaned tweet:\",cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cleaned=tweet.translate(str.maketrans(\"o\",\"^\"))\n",
    "print(\"Original tweet:\", tweet)\n",
    "print(\"Cleaned tweet:\",cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe Project Gutenberg EBook of The Complete Works of William Shakespeare, by\\n', 'William Shakespeare\\n', '\\n', 'This eBook is for the use of anyone anywhere at no cost and with\\n', 'almost no restrictions whatsoever.  You may copy it, give it away or\\n']\n"
     ]
    }
   ],
   "source": [
    "my_text=open(\"shakespeare.txt\", \"r\").readlines()\n",
    "print(my_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of 7\n",
      "the 7\n",
      "ebook 6\n",
      "shakespeare 5\n",
      "this 5\n",
      "project 4\n",
      "gutenberg 4\n",
      "william 4\n",
      "complete 3\n",
      "it 3\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punc=string.punctuation\n",
    "from collections import defaultdict\n",
    "#----------------------------------\n",
    "def get_dict(sentences):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "    input: @sentences: a list of sentences\n",
    "    returns: a dictionary of the words in the sentences.\n",
    "             dict key is a word and value is word frequency\n",
    "    \"\"\"\n",
    "    word_freq=defaultdict(int)\n",
    "    for sent in sentences:\n",
    "        sent= sent.translate(str.maketrans(\"\",\"\", punc))\n",
    "        words=sent.lower().split()\n",
    "        for w in words:\n",
    "            word_freq[w]+=1\n",
    "    return word_freq\n",
    "###############\n",
    "\n",
    "def sort_dict_ascendingly(d):\n",
    "    \"\"\"\n",
    "    Sorts by count/value of the \"freqs\" dictionary \n",
    "    in reverse order such that the highest values occur first \n",
    "    \"\"\"\n",
    "    list_tuples =sorted(d.items(), key = lambda x: x[1], reverse=True)\n",
    "    # lt -> [('of', 7), ('the', 7), ('ebook', 6), ('shakespeare', 5)]\n",
    "    return list_tuples\n",
    "\n",
    "#-----------------------------\n",
    "lines=open(\"shakespeare.txt\", \"r\").readlines()\n",
    "sentences=lines[:30]\n",
    "#-----------------------------\n",
    "freqs_dict=get_dict(sentences)\n",
    "lt= sort_dict_ascendingly(freqs_dict)\n",
    "#-----------------------------\n",
    "for i in lt:\n",
    "    w=i[0] # word\n",
    "    freq=i[-1] # freq of word\n",
    "    if w and freq > 2:\n",
    "        print(w, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This will write freqs to a file\n",
    "lines=open(\"shakespeare.txt\", \"r\").readlines()\n",
    "freqs_dict=get_dict(lines)\n",
    "lt= sort_dict_ascendingly(freqs_dict)\n",
    "word_freqs=open(\"./new_word_list.txt\", \"w\")\n",
    "for i in lt:\n",
    "    # For readability\n",
    "    w=i[0]\n",
    "    freq=i[-1]\n",
    "    word_freqs.write(w+\"\\t\"+str(freq)+\"\\n\")\n",
    "    \n",
    "word_freqs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t27824\r\n",
      "and\t26791\r\n",
      "i\t20681\r\n",
      "to\t19261\r\n",
      "of\t18289\r\n",
      "a\t14667\r\n",
      "you\t13716\r\n",
      "my\t12481\r\n",
      "that\t11135\r\n",
      "in\t11027\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 new_word_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identical\t1\r\n",
      "digits\t1\r\n",
      "digit\t1\r\n",
      "10234\t1\r\n",
      "httpwwwgutenbergorg102310234\t1\r\n",
      "24689\t1\r\n",
      "httpwwwgutenbergorg246824689\t1\r\n",
      "alternative\t1\r\n",
      "locating\t1\r\n",
      "httpwwwgutenbergorggutindexall\t1\r\n"
     ]
    }
   ],
   "source": [
    "!tail -10 new_word_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamlet\t107\r\n",
      "hamlets\t10\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"hamlet\" new_word_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\t2053\r\n",
      "loves\t286\r\n",
      "lovers\t74\r\n",
      "lovely\t53\r\n",
      "lover\t52\r\n",
      "loved\t47\r\n",
      "lovell\t40\r\n",
      "glove\t36\r\n",
      "beloved\t29\r\n",
      "gloves\t17\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"love\" new_word_list.txt | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\t2861\r\n",
      "being\t662\r\n",
      "nothing\t633\r\n",
      "bring\t444\r\n",
      "thing\t355\r\n",
      "things\t338\r\n",
      "kings\t284\r\n",
      "buckingham\t254\r\n",
      "something\t180\r\n",
      "bolingbroke\t175\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"ing\" new_word_list.txt | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\t2812\r\n",
      "too\t1232\r\n",
      "look\t828\r\n",
      "blood\t639\r\n",
      "poor\t626\r\n",
      "fool\t461\r\n",
      "bloody\t224\r\n",
      "looks\t214\r\n",
      "foot\t169\r\n",
      "took\t158\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"oo\" new_word_list.txt | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Take a look at the preface here: http://www.nltk.org/book/ch00.html\n",
    "* This tutorial is based on Python 2.7, but it shouldn't be an issue to write the same code for Python 3 as the differences are minimal so long as the tutorial is concerned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /Users/mam/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to /Users/mam/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to /Users/mam/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to /Users/mam/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n",
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('gutenberg')\n",
    "# nltk.download('genesis')\n",
    "# nltk.download('inaugural')\n",
    "# nltk.download('nps_chat')\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the free fellow our power opportunity of that by order on a years more\n",
      "necessary time other experienced duties own\n"
     ]
    }
   ],
   "source": [
    "text4.similar(\"patriotic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('man', 102)\n",
      "('woman', 3)\n",
      "('father', 4)\n",
      "('mother', 4)\n"
     ]
    }
   ],
   "source": [
    "# Counting word frequencies:\n",
    "words=[\"man\", \"woman\", \"father\", \"mother\"]\n",
    "for w in words:\n",
    "    print(w, text4.count(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States; fellow citizens; four years; years ago; Federal\n",
      "Government; General Government; American people; Vice President; Old\n",
      "World; Almighty God; Fellow citizens; Chief Magistrate; Chief Justice;\n",
      "God bless; every citizen; Indian tribes; public debt; one another;\n",
      "foreign nations; political parties\n"
     ]
    }
   ],
   "source": [
    "# Collocations\n",
    "text4.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mam/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'happy']\n",
      "['we', \"'re\", 'playing', 'tennis']\n",
      "['we', \"'ll\", 'study']\n",
      "['They', \"'ve\", 'cooked']\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization with NLTK:\n",
    "import nltk\n",
    "raw=[\"I'm happy\", \"we're playing tennis\", \"we'll study\",\\\n",
    "     \"They've cooked\"]\n",
    "for i in raw:\n",
    "    tokens=nltk.word_tokenize(i)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's look at the text4: Inaugural Address Corpus\n",
    "* NLTK can show a word in context, called a concordance (with a given text window size):\n",
    "* **width:** a parameter forthe window size of surrounding character context\n",
    "* **lines:** a parameter for the number of lines returned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 25 matches:\n",
      "ons may come . The Negroes are now Americans . Their ancestors came here years \n",
      " not . And yet we are not the less Americans on that account . We shall be the \n",
      "em now secure ; and there comes to Americans the profound assurance that our re\n",
      "d me . I am certain that my fellow Americans expect that on my induction into t\n",
      "urricanes of disaster . In this we Americans were discovering no wholly new tru\n",
      "freedom is an ebbing tide . But we Americans know that this is not true . Eight\n",
      "re not content to stand still . As Americans , we go forward , in the service o\n",
      "be simple and its words brief . We Americans of today , together with our allie\n",
      "charge of this responsibility , we Americans know and we observe the difference\n",
      "in of trading honor for security . Americans , indeed all free men , remember t\n"
     ]
    }
   ],
   "source": [
    "text4.concordance(\"Americans\", width=80, lines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org', '**', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', ',', 'details', 'below', '**', '**', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', '.', '**', 'title', ':', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'author', ':', 'william', 'shakespeare', 'posting', 'date', ':', 'september']\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from nltk import word_tokenize, Text\n",
    "text_string=codecs.open(\"shakespeare.txt\", \"r\", \"utf-8\").read() # Opens for reading and gets you the file content as a list\n",
    "text_string=text_string.lower()\n",
    "tokens = word_tokenize(text_string)\n",
    "print(type(tokens))\n",
    "print(tokens[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "['project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org', '**', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', ',', 'details', 'below', '**', '**', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', '.', '**', 'title', ':', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'author', ':', 'william', 'shakespeare', 'posting', 'date', ':', 'september']\n"
     ]
    }
   ],
   "source": [
    "text = Text(tokens)\n",
    "print(\"*\"*50)\n",
    "print(text[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mam/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "william shakespeare; thou art; project gutenberg; illinois\n",
      "benedictine; machine readable; prohibited commercial; benedictine\n",
      "college; commercial distribution; distribution includes; copyright\n",
      "1990-1993; used commercially; king henry; complete works; gutenberg\n",
      "etext; electronic version; readable copies; others personal; personal\n",
      "use; world library; thou hast\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(text.collocations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mam/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/mam/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hamlet: Entire Play\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Tragedy of Hamlet, Prince of Denmark\n",
      "\n",
      "Shakespeare homepage \n",
      "    | Hamlet \n",
      "    | Entire play\n",
      "\n",
      "ACT I\n",
      "SCENE I. Elsinore. A platform before the castle.\n",
      "\n",
      "FRANCISCO at his post. Enter to him BERNARDO\n",
      "\n",
      "BERNARDO\n",
      "\n",
      "Who's there?\n",
      "\n",
      "FRANCISCO\n",
      "\n",
      "Nay, answer me: stand, and unfold you\n"
     ]
    }
   ],
   "source": [
    "# Fetching and cleaning a webpage:\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"http://shakespeare.mit.edu/hamlet/full.html\"\n",
    "page = urlopen(url)\n",
    "soup = BeautifulSoup(page.read())   \n",
    "raw = BeautifulSoup.get_text(soup)  \n",
    "print(raw[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hamlet', ':', 'entire', 'play', 'the', 'tragedy', 'of', 'hamlet', ',', 'prince', 'of', 'denmark', 'shakespeare', 'homepage', '|', 'hamlet', '|', 'entire', 'play', 'act']\n"
     ]
    }
   ],
   "source": [
    "raw=raw.lower()\n",
    "tokens=nltk.word_tokenize(raw)\n",
    "print(tokens[:20])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
